{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering based Recommender System From Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MovieLens 100K Dataset\n",
    "\n",
    "There are a number of datasets available for recommendations systems research. Amongst them, the [MovieLens](https://grouplens.org/datasets/movielens) dataset is probably one of the more popular ones. MovieLens is a non-commercial web-based movie recommender system, created in 1997 and run by the GroupLens Research Project, at University of Minnesota. It's data has been critical for several research studies including personalized recommendation and social psychology.\n",
    "\n",
    "There are several versions of the dataset available. We'll use the well-known [MovieLens 100k](https://grouplens.org/datasets/movielens/100k/), which consists of 100,000 ratings form 943 users on 1682 movies. Some simple demographic information such as age, gender, genres for the users and items are also available, but we'll not use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-23 07:37:08--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4924029 (4,7M) [application/zip]\n",
      "Saving to: ‘ml-100k.zip’\n",
      "\n",
      "ml-100k.zip         100%[===================>]   4,70M  2,01MB/s    in 2,3s    \n",
      "\n",
      "2024-10-23 07:37:11 (2,01 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
      "\n",
      "Archive:  ml-100k.zip\n",
      "   creating: ml-100k/\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(\"ml-100k\"):\n",
    "    # # Download the dataset\n",
    "    !wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "    # # Unzip it\n",
    "    !unzip ml-100k.zip\n",
    "    # # Get rid of the .zip file\n",
    "    !rm ml-100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset folder contains several files, but we need only two of them:\n",
    "\n",
    "* **u.data**: The full dataset with 100,000 user ratings. Each user has rated at least 20 movies.\n",
    "* **u.item**: Contains informations about the movies, but we will use only movies ids and titles. This data is not required for the understanding of the CF technique, but we will use it for a more friendly feedback of our system.\n",
    "\n",
    "Let's begin by loading the ratings dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>880</td>\n",
       "      <td>476</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>716</td>\n",
       "      <td>204</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>276</td>\n",
       "      <td>1090</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>13</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>12</td>\n",
       "      <td>203</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  movie_id  rating\n",
       "0          196       242       3\n",
       "1          186       302       3\n",
       "2           22       377       1\n",
       "3          244        51       2\n",
       "4          166       346       1\n",
       "...        ...       ...     ...\n",
       "99995      880       476       3\n",
       "99996      716       204       5\n",
       "99997      276      1090       1\n",
       "99998       13       225       2\n",
       "99999       12       203       3\n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\n",
    "    \"ml-100k/u.data\",\n",
    "    sep=\"\\t\", # this is a tab separated data\n",
    "    names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"], # the columns names\n",
    "    usecols=[\"user_id\", \"movie_id\", \"rating\"], # we do not need the timestamp column\n",
    "    low_memory=False\n",
    ")\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry consists in a *user_id*, *movie_id* and a movie *rating* in range of 1 to 5.\n",
    "\n",
    "The next information we need is the number of movies and users in the dataset. Although we already know this from the dataset information, for didactic purposes we'll get them from the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 943 -- Number of movies: 1682\n"
     ]
    }
   ],
   "source": [
    "n_users = len(ratings.user_id.unique())\n",
    "n_movies = len(ratings.movie_id.unique())\n",
    "\n",
    "print(f\"Number of users: {n_users} -- Number of movies: {n_movies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "Now we'll create the train and test sets that we will use to evaluate the performance of our system. 20% of each user ratings will be used for testing, and the remaining that will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 3), (20000, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_split = 0.2 #percent of data to be used for testing\n",
    "\n",
    "# Initialize the train and test dataframes.\n",
    "train_set, test_set = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# Check each user.\n",
    "for user_id in ratings.user_id.unique():\n",
    "    user_df = ratings[ratings.user_id == user_id].sample(\n",
    "        frac=1,\n",
    "        random_state=42\n",
    "    ) # select only samples of the actual user and shuffle the resulting dataframe\n",
    "    \n",
    "    n_entries = len(user_df)\n",
    "    n_test = int(round(test_split * n_entries))\n",
    "    \n",
    "    test_set = pd.concat((test_set, user_df.tail(n_test)))\n",
    "    train_set = pd.concat((train_set, user_df.head(n_entries - n_test)))\n",
    "\n",
    "train_set = train_set.sample(frac=1).reset_index(drop=True)\n",
    "test_set = test_set.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function to compute the *interactions matrix* of a given ratings dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_interactions_matrix(r_mat, n_users, n_items):\n",
    "    iter_m = np.zeros((n_users, n_items))\n",
    "    \n",
    "    for _, user_id, movie_id, rating in r_mat.itertuples():\n",
    "        iter_m[user_id-1, movie_id-1] = rating\n",
    "    \n",
    "    return iter_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_m = build_interactions_matrix(ratings, n_users, n_movies)\n",
    "iter_m.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity measurement\n",
    "In this post we'll build our system using the **memory based** approach, in which similarities between users/items are computed using the rating data itself. Therefore, the *i*-th row of an interactions matrix is considered as the **feature vector** of user *i*, while the *j*-th column of an interaction matrix is considered as the **feature vector** of item *j*.\n",
    "\n",
    "The similarity between two users is represented by some **distance measurement** between their feature vectors. Multiple measures, such as Pearson correlation and vector cosine are used for this. For example, the similarity between users $u$ and $u'$ can be computed using vector cosine as:\n",
    "$$\n",
    "sim(u, u') = cos(\\textbf{r}_u, \\textbf{r}_{u'}) = \n",
    "\\frac{\\textbf{r}_u \\textbf{.} \\textbf{r}_{u'}}{|\\textbf{r}_u||\\textbf{r}_{u'}|} =\n",
    "\\frac{\\sum_i r_{ui}r_{u'i}}{\\sqrt{\\sum_i r_{ui}^2}\\sqrt{\\sum_i r_{u'i}^2}}\n",
    "$$\n",
    "where $\\textbf{r}_u$ and $\\textbf{r}_{u'}$ are the feature vectors of users $u$ and $u'$, respectively, and $r_{ui}$ is a rating value given by user $u$ to item $i$. The same procedure is applied when computing the similarity between items $i$ and $i'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(interactions_matrix, kind=\"user\", eps=1e-9):\n",
    "    # takes rows as user features\n",
    "    if kind == \"user\":\n",
    "        similarity_matrix = interactions_matrix.dot(interactions_matrix.T)\n",
    "    # takes columns as item features\n",
    "    elif kind == \"item\":\n",
    "        similarity_matrix = interactions_matrix.T.dot(interactions_matrix)\n",
    "    norms = np.sqrt(similarity_matrix.diagonal()) + eps\n",
    "    return similarity_matrix / (norms[np.newaxis, :] * norms[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User similarity matrix shape: (943, 943)\n",
      "User similarity matrix sample:\n",
      "[[1.         0.16693098 0.04745954 0.06435782]\n",
      " [0.16693098 1.         0.11059132 0.17812119]\n",
      " [0.04745954 0.11059132 1.         0.34415072]\n",
      " [0.06435782 0.17812119 0.34415072 1.        ]]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Item similarity matrix shape: (1682, 1682)\n",
      "Item similarity matrix sample:\n",
      "[[1.         0.40238218 0.33024479 0.45493792]\n",
      " [0.40238218 1.         0.27306918 0.50257077]\n",
      " [0.33024479 0.27306918 1.         0.32486639]\n",
      " [0.45493792 0.50257077 0.32486639 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "u_sim = build_similarity_matrix(iter_m, kind=\"user\")\n",
    "i_sim = build_similarity_matrix(iter_m, kind=\"item\")\n",
    "\n",
    "print(f\"User similarity matrix shape: {u_sim.shape}\\nUser similarity matrix sample:\\n{u_sim[:4, :4]}\")\n",
    "print(\"-\" * 97)\n",
    "print(f\"Item similarity matrix shape: {i_sim.shape}\\nItem similarity matrix sample:\\n{i_sim[:4, :4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity matrix is a symmetric matrix with values in range 0 to 1. The diagonal elements contains the auto-similarities of all users/items, so all elements are equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this method alone can improve greatly our system's prediction power. Later in this post, we'll try leverage the effect of the number of neighbors to do a simple tunning in our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions with bias subtraction\n",
    "\n",
    "Now we are able to make predictions. Depending on which approach we have chosen for our system, we have two different objectives:\n",
    "\n",
    "1. If we choose the user-based approach, we'll infer a missing rating $r_{ui}$ of an user $u$ to an item $i$ by taking the normalized weighted sum of **all ratings of other users to this item**.\n",
    "\n",
    "$$\n",
    "r_{ui} = \\frac{\\sum_{u'} sim(u, u')r_{u'i}}{\\sum_{u'} |sim(u, u')|}\n",
    "$$\n",
    "\n",
    "2. If we choose the item-based approach instead, we'll infer a missing rating $r_{ui}$ of an user $u$ to an item $i$ by taking the normalized weighted sum of **all other ratings of this user to the other items**.\n",
    "\n",
    "$$\n",
    "r_{ui} = \\frac{\\sum_{i'} sim(i, i')r_{ui'}}{\\sum_{i'} |sim(i, i')|}\n",
    "$$\n",
    "\n",
    "Now we'll try to deal with the rating bias associated with an user or an item. The ideia here is that certain users may tend to always give high or low ratings to all movies, so the *relative difference* in ratings may be more important than the *absolute rating* values.\n",
    "\n",
    "For a user-based approach this methodology can be mathematically described as:\n",
    "\n",
    "$$\n",
    "r_{ui} = \\overline{r}_{u} + \\frac{\\sum_{u'} sim(u, u')(r_{u'i} - \\overline{r}_{u'})}{\\sum_{u'} |sum(u, u')|}\n",
    "$$\n",
    "\n",
    "where $\\overline{r}_{u}$ is the average rating given by user *u*, or for a item-based approach as:\n",
    "\n",
    "$$\n",
    "r_{ui} = \\overline{r}_{i} + \\frac{\\sum_{i'} sim(i, i')(r_{ui'} - \\overline{r}_{i'})}{\\sum_{i'} |sum(i, i')|}\n",
    "$$\n",
    "\n",
    "where $\\overline{r}_{i}$ is the average rating of item *i*\n",
    "\n",
    "Lets modify our *Recommender* class once more to include this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_users,\n",
    "        n_items,\n",
    "        r_mat,\n",
    "        k=40,\n",
    "        kind=\"user\",\n",
    "        bias_sub=False,\n",
    "        eps=1e-9\n",
    "    ):\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.kind = kind\n",
    "        self.iter_m = build_interactions_matrix(r_mat, self.n_users, self.n_items)\n",
    "        self.sim_m = build_similarity_matrix(self.iter_m, kind=self.kind)\n",
    "        self.bias_sub = bias_sub\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.predictions = self._predict_all()\n",
    "    \n",
    "    def _predict_all(self):\n",
    "        pred = np.empty_like(self.iter_m)\n",
    "        if self.kind == \"user\":\n",
    "            # Computes the new interaction matrix if needed.\n",
    "            iter_m = self.iter_m\n",
    "            if self.bias_sub:\n",
    "                user_bias = self.iter_m.mean(axis=1)[:, np.newaxis]\n",
    "                iter_m -= user_bias\n",
    "            # An user has the higher similarity score with itself,\n",
    "            # so we skip the first element.\n",
    "            sorted_ids = np.argsort(-self.sim_m)[:, 1:self.k+1]\n",
    "            for user_id, k_users in enumerate(sorted_ids):\n",
    "                pred[user_id, :] = self.sim_m[user_id, k_users].dot(iter_m[k_users, :])\n",
    "                pred[user_id, :] /= \\\n",
    "                    np.abs(self.sim_m[user_id, k_users] + self.eps).sum() + self.eps\n",
    "            if self.bias_sub:\n",
    "                pred += user_bias\n",
    "            \n",
    "        elif self.kind == \"item\":\n",
    "            # Computes the new interaction matrix if needed.\n",
    "            iter_m = self.iter_m\n",
    "            if self.bias_sub:\n",
    "                item_bias = self.iter_m.mean(axis=0)[np.newaxis, :]\n",
    "                iter_m -= item_bias\n",
    "            # An item has the higher similarity score with itself,\n",
    "            # so we skip the first element.\n",
    "            sorted_ids = np.argsort(-self.sim_m)[:, 1:self.k+1]\n",
    "            for item_id, k_items in enumerate(sorted_ids):\n",
    "                pred[:, item_id] = self.sim_m[item_id, k_items].dot(iter_m[:, k_items].T)\n",
    "                pred[:, item_id] /= \\\n",
    "                    np.abs(self.sim_m[item_id, k_items] + self.eps).sum() + self.eps\n",
    "            if self.bias_sub:\n",
    "                pred += item_bias\n",
    "                \n",
    "        return pred.clip(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_predictions_df(preds_m, dataframe):\n",
    "    preds_v = []\n",
    "    for row_id, user_id, movie_id, _ in dataframe.itertuples():\n",
    "        preds_v.append(preds_m[user_id-1, movie_id-1])\n",
    "    preds_df = pd.DataFrame(data={\"user_id\": dataframe.user_id, \"movie_id\": dataframe.movie_id, \"rating\": preds_v})\n",
    "    return preds_df\n",
    "\n",
    "def get_mse(estimator, train_set, test_set):\n",
    "    train_preds = build_predictions_df(estimator.predictions, train_set)\n",
    "    test_preds = build_predictions_df(estimator.predictions, test_set)\n",
    "    \n",
    "    train_mse = mean_squared_error(train_set.rating, train_preds.rating)\n",
    "    test_mse = mean_squared_error(test_set.rating, test_preds.rating)\n",
    "    \n",
    "    return train_mse, test_mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based train MSE: 5.6854492668354295 -- User-based test MSE: 6.39456526201878\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Item-based train MSE: 5.4271747547608555 -- Item-based test MSE: 6.214109193478462\n"
     ]
    }
   ],
   "source": [
    "train_mse, test_mse = get_mse(\n",
    "    Recommender(n_users, n_movies, train_set, kind=\"user\", bias_sub=True),\n",
    "    train_set,\n",
    "    test_set\n",
    ")\n",
    "\n",
    "print(f\"User-based train MSE: {train_mse} -- User-based test MSE: {test_mse}\")\n",
    "print(\"-\" * 97)\n",
    "\n",
    "train_mse, test_mse = get_mse(\n",
    "    Recommender(n_users, n_movies, train_set, kind=\"item\", bias_sub=True),\n",
    "    train_set,\n",
    "    test_set\n",
    ")\n",
    "\n",
    "print(f\"Item-based train MSE: {train_mse} -- Item-based test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this methodology did not improved the results for this scenario, possibly due to the characteristics of the dataset, it can be effective with another dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning up\n",
    "\n",
    "There is one question left: how do we find the right number of similar users/items we should use when predicting a rating?\n",
    "\n",
    "Our use case is quite simple, as we have only two parameters (*k* and *bias_sub*). I have run an optimization on objective ab found the best values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrieve the best parameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      "  - k = 12\n",
      "  - bias_sub = False\n"
     ]
    }
   ],
   "source": [
    "best_k = 12\n",
    "best_bias_sub = False\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(f\"  - k = {best_k}\")\n",
    "print(f\"  - bias_sub = {best_bias_sub}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item recommendation\n",
    "\n",
    "Now that we have defined our parameters, we can make our system do what it is suposed to: recommend items. There are many ways to acomplish this, but I optioned to go for the simple way and just recommend items most similar to a item using a *item-based* system.\n",
    "\n",
    "Let's make the last modification to our *Recommender* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_users,\n",
    "        n_items,\n",
    "        r_mat,\n",
    "        k=40,\n",
    "        kind=\"user\",\n",
    "        bias_sub=False,\n",
    "        eps=1e-9\n",
    "    ):\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.kind = kind\n",
    "        self.iter_m = build_interactions_matrix(r_mat, self.n_users, self.n_items)\n",
    "        self.sim_m = build_similarity_matrix(self.iter_m, kind=self.kind)\n",
    "        self.bias_sub = bias_sub\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.predictions = self._predict_all()\n",
    "    \n",
    "    def _predict_all(self):\n",
    "        pred = np.empty_like(self.iter_m)\n",
    "        if self.kind == \"user\":\n",
    "            # Computes the new interaction matrix if needed.\n",
    "            iter_m = self.iter_m\n",
    "            if self.bias_sub:\n",
    "                user_bias = self.iter_m.mean(axis=1)[:, np.newaxis]\n",
    "                iter_m -= user_bias\n",
    "            # An user has the higher similarity score with itself,\n",
    "            # so we skip the first element.\n",
    "            sorted_ids = np.argsort(-self.sim_m)[:, 1:self.k+1]\n",
    "            for user_id, k_users in enumerate(sorted_ids):\n",
    "                pred[user_id, :] = self.sim_m[user_id, k_users].dot(iter_m[k_users, :])\n",
    "                pred[user_id, :] /= np.abs(self.sim_m[user_id, k_users]).sum() + self.eps\n",
    "            if self.bias_sub:\n",
    "                pred += user_bias\n",
    "            \n",
    "        elif self.kind == \"item\":\n",
    "            # Computes the new interaction matrix if needed.\n",
    "            iter_m = self.iter_m\n",
    "            if self.bias_sub:\n",
    "                item_bias = self.iter_m.mean(axis=0)[np.newaxis, :]\n",
    "                iter_m -= item_bias\n",
    "            # An item has the higher similarity score with itself,\n",
    "            # so we skip the first element.\n",
    "            sorted_ids = np.argsort(-self.sim_m)[:, 1:self.k+1]\n",
    "            for item_id, k_items in enumerate(sorted_ids):\n",
    "                pred[:, item_id] = self.sim_m[item_id, k_items].dot(iter_m[:, k_items].T)\n",
    "                pred[:, item_id] /= np.abs(self.sim_m[item_id, k_items]).sum() + self.eps\n",
    "            if self.bias_sub:\n",
    "                pred += item_bias\n",
    "                \n",
    "        return pred.clip(0, 5)\n",
    "    \n",
    "    def get_top_recomendations(self, item_id, n=6):\n",
    "        if self.kind == \"user\":\n",
    "            # For an user-based system, only similarities between users were computed.\n",
    "            # This strategy will not be covered in this post, but a solution to this\n",
    "            # could be of finding the top better rated items of similiar users.\n",
    "            # I'll leave this exercise to you =]\n",
    "            pass\n",
    "        if self.kind == \"item\":\n",
    "            sim_row = self.sim_m[item_id - 1, :]\n",
    "            # once again, we skip the first item for obviouos reasons.\n",
    "            items_idxs = np.argsort(-sim_row)[1:n+1]\n",
    "            similarities = sim_row[items_idxs]\n",
    "            return items_idxs + 1, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added a method to return the $n$ most similar items to a given item. Now, we just need to buil our model with the parameters found previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.2755618709089025, 3.2546408150520763)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_model = Recommender(\n",
    "    n_users, \n",
    "    n_movies, \n",
    "    ratings, # the model will be built on the full dataset now\n",
    "    k=best_k, \n",
    "    kind=\"item\", \n",
    "    bias_sub=best_bias_sub\n",
    ")\n",
    "get_mse(rs_model, train_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define two functions: one that maps a movie title to an id and other that maps a list of movie ids into a list of movie titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title2id(mapper_df, movie_title):\n",
    "    return mapper_df.loc[mapper_df.movie_title == movie_title, \"movie_title\"].index.values[0]\n",
    "\n",
    "def ids2title(mapper_df, ids_list):\n",
    "    titles = []\n",
    "    for id in ids_list:\n",
    "        titles.append(mapper_df.loc[id, \"movie_title\"])\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those functions need a dataframe with the movies ids and titles, that will act as a mapper. So we'll load the **u.item** file from the dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GoldenEye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Four Rooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Get Shorty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Copycat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>Mat' i syn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>B. Monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>Sliding Doors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>You So Crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>Scream of Stone (Schrei aus Stein)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1682 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 movie_title\n",
       "movie_id                                    \n",
       "1                                  Toy Story\n",
       "2                                  GoldenEye\n",
       "3                                 Four Rooms\n",
       "4                                 Get Shorty\n",
       "5                                    Copycat\n",
       "...                                      ...\n",
       "1678                              Mat' i syn\n",
       "1679                               B. Monkey\n",
       "1680                           Sliding Doors\n",
       "1681                            You So Crazy\n",
       "1682      Scream of Stone (Schrei aus Stein)\n",
       "\n",
       "[1682 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns names\n",
    "movies_mapper_cols = [\n",
    "    \"movie_id\", \n",
    "    \"movie_title\", \n",
    "    \"release_date\", \n",
    "    \"video_release_date\", \n",
    "    \"IMDb_URL\", \n",
    "    \"unknown\",\n",
    "    \"Action\",\n",
    "    \"Adventure\",\n",
    "    \"Animation\",\n",
    "    \"Childrens\",\n",
    "    \"Comedy\",\n",
    "    \"Crime\",\n",
    "    \"Documentary\",\n",
    "    \"Drama\",\n",
    "    \"Fantasy\",\n",
    "    \"Film_Noir\",\n",
    "    \"Horror\",\n",
    "    \"Musical\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Sci_Fi\",\n",
    "    \"Thriller\",\n",
    "    \"War\",\n",
    "    \"Western\" \n",
    "]\n",
    "movies_mapper = pd.read_csv(\n",
    "    \"ml-100k/u.item\",\n",
    "    sep=\"|\",\n",
    "    encoding=\"latin\",\n",
    "    names=movies_mapper_cols,\n",
    "    usecols=[\"movie_id\", \"movie_title\"], # we only need these columns\n",
    "    index_col=\"movie_id\"\n",
    ")\n",
    "# Remove movies release years from titles\n",
    "movies_mapper[\"movie_title\"] = movies_mapper[\"movie_title\"].apply(\n",
    "    lambda title: re.sub(\"\\(\\d{4}\\)\", \"\", title).strip()\n",
    ")\n",
    "movies_mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make our recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommendations(model, mapper, movie_title):\n",
    "    ids_list, similarities = rs_model.get_top_recomendations(title2id(mapper, movie_title))\n",
    "    titles = ids2title(movies_mapper, ids_list)\n",
    "    for title, similarity in zip (titles, similarities):\n",
    "        print(f\"{similarity:.2f} -- {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73 -- Star Wars\n",
      "0.70 -- Return of the Jedi\n",
      "0.69 -- Independence Day (ID4)\n",
      "0.66 -- Rock, The\n",
      "0.64 -- Mission: Impossible\n",
      "0.64 -- Willy Wonka and the Chocolate Factory\n"
     ]
    }
   ],
   "source": [
    "print_recommendations(rs_model, movies_mapper, \"Toy Story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71 -- Batman\n",
      "0.64 -- Batman Forever\n",
      "0.62 -- Stargate\n",
      "0.62 -- Die Hard: With a Vengeance\n",
      "0.61 -- True Lies\n",
      "0.61 -- Crow, The\n"
     ]
    }
   ],
   "source": [
    "print_recommendations(rs_model, movies_mapper, \"Batman Returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66 -- Under Siege\n",
      "0.62 -- Top Gun\n",
      "0.62 -- True Lies\n",
      "0.62 -- Batman\n",
      "0.60 -- Stargate\n",
      "0.60 -- Cliffhanger\n"
     ]
    }
   ],
   "source": [
    "print_recommendations(rs_model, movies_mapper, \"GoldenEye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70 -- Star Wars\n",
      "0.67 -- Godfather: Part II, The\n",
      "0.65 -- Fargo\n",
      "0.63 -- Return of the Jedi\n",
      "0.59 -- Raiders of the Lost Ark\n",
      "0.58 -- Pulp Fiction\n"
     ]
    }
   ],
   "source": [
    "print_recommendations(rs_model, movies_mapper, \"Godfather, The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50 -- Dumb & Dumber\n",
      "0.49 -- Ace Ventura: Pet Detective\n",
      "0.45 -- Hot Shots! Part Deux\n",
      "0.44 -- Brady Bunch Movie, The\n",
      "0.44 -- Young Guns II\n",
      "0.43 -- Tommy Boy\n"
     ]
    }
   ],
   "source": [
    "print_recommendations(rs_model, movies_mapper, \"Billy Madison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75 -- Aladdin\n",
      "0.69 -- Beauty and the Beast\n",
      "0.68 -- Forrest Gump\n",
      "0.66 -- Jurassic Park\n",
      "0.65 -- E.T. the Extra-Terrestrial\n",
      "0.65 -- Empire Strikes Back, The\n"
     ]
    }
   ],
   "source": [
    "print_recommendations(rs_model, movies_mapper, \"Lion King, The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88 -- Return of the Jedi\n",
      "0.76 -- Raiders of the Lost Ark\n",
      "0.75 -- Empire Strikes Back, The\n",
      "0.73 -- Toy Story\n",
      "0.70 -- Godfather, The\n",
      "0.69 -- Independence Day (ID4)\n"
     ]
    }
   ],
   "source": [
    "print_recommendations(rs_model, movies_mapper, \"Star Wars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook of this post can found [here](https://github.com/TheCamilovisk/DSNotebooks/blob/main/RecommenderSystems/CollaborativeFilteringMemoryBased.ipynb).\n",
    "\n",
    "For ensemble learning please see the blog post [here](https://www.kaggle.com/code/satishgunjal/ensemble-learning-bagging-boosting-stacking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
